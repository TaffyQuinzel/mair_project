\section{Theoretical Objections}

Alan Turing proposed this experiment to stop the endless discussion about the meaning of the words \textit{think} and \textit{machines}. But, ironically enough, his paper only generated more discussion among philosophers~\cite{dennett2004can}. In this section, we will discuss two famous and often mentioned theoretical objections that argue against the idea that the Turing Test measures intelligence in machines.

\subsection{Chinese Room Argument}
The first theoretical objection, which is probably the most famous objection against the Turing Test, is known as the Chinese Room argument. This argument comes from American philosopher John Searle and is described in his paper called \textit{Mind, brains, and programs}~\cite{searle1980minds}. Searle lets us suppose that we have a computer program called \textit{Sam} which is able to answer Chinese questions back in Chinese. We now replace this program \textit{Sam} by a human \textit{Joe}, and we envision this situation as if Joe is sitting in a room detached from the rest of the world. Joe is equipped with instruction books on the program instructions, thus the books describe which character to add in Joe's answer when a specific character is present in the question. Joe is American and he does not understand anything from Chinese. Therefore, Joe has no idea what the questions or answers mean. However, to an external observer, it looks like he can speak Chinese since he gives reasonable answers to the questions.

The claim Searle wants to make with this thought experiment is that executing a program does not imply any understanding of what the program is doing or attaching meaning to the Chinese symbols which are used. A computer program uses symbols (zeros and ones) in a similar way, thus is also not capable of understanding these symbols or of attaching meaning to the symbols. Because the Turing Test only uses words, which can be interpreted as symbols, the test does not test real understanding.


\subsection{Simulation Objection}
The second theoretical objection is the simulation objection. This objection states that a simulated \textit{X} is not yet an \textit{X}. That means that even when a computer would pass the test it might only give a good simulation of thinking but that it is still not actually thinking. Consider the original Imitation Game in which an interrogator has to distinguish between a man and a woman. When the interrogator thinks that the man is a woman, this is a clear case of a wrong identification. But we wouldn't say that the man is in fact a woman. We would only want to say that during the conversation the man gave a good simulation of a woman.

According to Copeland~\cite{copeland2015artificial}, there are two ways in which we can understand something as a simulation. The first way is when a simulation lacks essential features of whatever is being simulated. He gives the example of a simulated death, which lacks certain features since the person who is having the simulation is still alive. The other way of understanding a simulation is when it is exactly like what is simulated except that it is hasn't been produced in the natural way. The example he gives here is of coal, which can be artificially produced, but still indistinguishable from naturally occurring coal.
Now the simulation objection assumes that computer simulations of thinking will always be mere simulations, never the real thing. When they assume that those simulations are never the \textit{real thing}, they actually mean (according to Copeland) that there are certain properties lacking like in the first understanding of simulation. But there is no good argument that simulations of thinking will always have to lack properties of thinking. They might as well need to be understood in the second sense in which the simulation of thinking can be indistinguishable from thinking even though it is produced in an artificial manner. There is still an ongoing debate in philosophy about which of those two ways of simulations of thinking we are actually measuring when a computer passes the Turing Test.


\section{Practical Objections}
The two discussed objections so far are deeply philosophical objections that question whether the Turing Test measures machine intelligence at all. The Chinese Room argument argued that a computer can pass the test while it has no understanding of the concepts it is using. The simulation objection argued that a computer that passes the test can give a good simulation of thinking, but that it is not actually thinking. Up until now, no agreement has been reached and the philosophical debate is going on.

Even though we might agree with the theoretical objections given above, we can assume that no other tests for machine intelligence can give appropriate answers to these philosophical objections. From a practical perspective, we might want to hold on to the idea of language as an important feature to distinguish human-like intelligence. Therefore, we might want to consider keeping the Turing Test and passing the test as a practical goal in AI research. In this section, we focus on the more practical objections that question whether the Turing Test could be practically used in science.

\subsection{The Test Is Too Hard}
The first practical objection is that the Turing Test is too hard for a computer to pass. Right now, many people think that we are so far away from passing the test that the goal of passing it might not be a realistic goal in AI research. Some people even think that no machine that man creates will ever pass the test.

One of the reasons that the test is too hard was given by Robert M. French who thinks that nothing without a \textit{human subcognitive substrate} could ever pass the test~\cite{french1996subcognition}. According to French, there are obvious questions that people can use to discriminate between humans and computers, which reveal in his words \textit{low-level cognitive structure}. By low-level cognitive structure he means the subconscious associative network in human minds. Humans develop many associations during their lives: they learn through experience that certain words or concepts are more commonly used together with second words or concepts than others, for example the words \textit{bread} and \textit{butter} in comparison with \textit{bread} and \textit{dog}. The interrogator in the Turing Test can make use of this associative network that he shares with other humans while he doesn't share this (at least not extensively) with the computer.

French expands on these ideas and introduced \textit{rating games} which are intentionally designed to be able to distinguish between humans and computers. One of his examples is that an interrogator would ask the participants to rate the name \textit{flugbots} as an appropriate name for a breakfast cereal. For the human participant in the game (who must be a native English speaker) the name \textit{flugbots} would unconsciously activates certain associations. Therefore, most English speakers agree that such a name would not be an appropriate name for a breakfast cereal. For the computer, on the other hand, there would not be such an extensive associative network present that allow it to come up with a similar answer to the human.

In the paper Imitation Versus Communication: Testing for Human-Like Intelligence Jamie Cullen adds in his paper that (other than neuro-associative representational differences) the additional relevance of physical embodiment differences~\cite{cullen2009imitation}. He gives the example of an interrogator who asks the participants to explain the meaning of \textit{I feel sick to my stomach}. Such questions would be extremely difficult to process for a machine, since the machines obviously does not have a digestive system. We can imagine that a machine can mimic typical human responses to these kind of questions. But such simulations could, according to Cullen, be unveiled by further questioning with the intention to unveil the weakness of the simulation.

A third (more general) reason that the test is too hard is that it does not only measure intelligence, but also how human-like the machine is~\cite{sep-turing-test}. It might be so that it is particularly hard to simulate certain human features, which have nothing or little to do with intelligence. Humor and making mistakes would be two examples of typical human features. While humor might be to a certain degree related to intelligence, making mistakes obviously is not related to intelligence. These typical human features could be easy indications that someone is talking with a person instead of a computer. If it turns out to be true that certain human features are extraordinarily hard to replicate in a machine, then passing the Turing Test is not an appropriate goal for future AI research.


\subsection{The Test Is Too Narrow}
The second practical objection is not aimed at the difficulty of the test, but at the scope of the test. It states that the Turing Test is too narrow to test intelligence. Many have argued that success in the test is not solely an indication of possessing intelligence. They think that it is therefore possible that non-intelligent beings pass the test. Philosopher Gerald J. Erion~\cite{erion2001cartesian} thus argues that while computers might be able to pass the test, they can still not do much else than the limited tasks involved in passing the test. They are, according to him, \textit{unable to act skilfully in the diverse range of situations that a person with common sense can}. During the Turing Test, computers are only answering questions via a chat interface, which is a very limited task. His argument is that outside of this limited communication task computers can't do many other tasks that we would call intelligent. We question whether that argument is correct. When the computer passes the test, it must be able to solve a wide variety of every circumstances, related to common knowledge, memory, personal identity, and many more. For example, the computer must have a storage memory of everything an interrogator earlier said in a conversation, otherwise people would immediately notice they are not talking with a person. Although the Turing Test is text-only, it thus requires the computer to do many subtasks which contribute to the computer's credibility of being a human.
