\section{Discussion}
Our research question was: \textit{How can and should a Turing Test be used in future Artificial Intelligence research?} We have addressed this question by performing both a literature review and in executing an experiment. In this section, we will discuss both the experiment itself and how the outcome of the experiment relates to the literature review.

\subsection{Experiment Discussion}
In this experiment, we tested the hypothesis that the introduction of generated spelling errors would allow a chatbot to perform better in a Turing test. Contrary to our hypothesis, our results did not yield a significant effect. The aim of this experiment discussion is to offer a number of methodological and theoretical arguments, as to why the results did not match our hypothesis. First, the spelling errors were automatically generated, and the amount of errors was set relative to the length of the chatbot's response. We argue that there is a fine line between introducing too many spelling errors, and too few. In the former case, the chatbot's response might be perceived as unreliable, while in the latter case, no effect will come of introducing the spelling errors, as they are too few to notice. As part of future work, pre-tests should be run so that an \textit{optimal} amount of introduced spelling errors might be found.           Furthermore, the sample size in our experiment was rather small. 20 participants is not enough to measure the effect of a relatively small manipulation in an experiment with 4 conditions. Future experiments would therefore benefit from a larger set of participants.

As part of the data analysis, we have found a strong positive correlation between prior knowledge of the Turing Test and the participants' confidence in identifying the computer as the computer. It seems that participants who had prior knowledge of the test were better equipped to test the chatbots, for example by asking questions that more effectively single out the computer; another example is that those participants were not surprised by the chatbot's ability to parse informal language, as were some of the participants less familiar with the Test and AI in general. For future work, considering the participants' knowledge of the test might prove important when designing the experiment and analyzing the results.

We learn from the participants' responses in the questionnaire, that a number of participants did find the chatbots more human when spelling errors were introduced see figure \textbf{(number of figure with conversation)} (\textit{Person 2 made a typo [that's why I think he's the human]}). Conversely, some participants found the computer's perfectly punctuated and spelled responses as a telltale of the computer's identity see figure (number of figure with conversation) (\textit{the wording of person 2's answers and the perfect punctuation made me feel like they were a computer}). A recurring theme found in the participants' responses, is that they found their questions ill understood by the chatbots see figure (number of figure with conversation)(\textit{person 2 did not answer the last question correctly}), or the replies given to them odd see figure (number of figure with conversation)(\textit{Person 2 was the computer because he did not communicate in a logical manner. His answers were quite weird}) or out of place see figure \textbf{(number of figure with conversation)} (\textit{I don't think someone would respond "I'm happy to know you're doing fine" in a casual chat conversation..}). Further, it is evident by these responses that the chatbots' behaviour heavily influenced their attempt to pass as humans as part of the test.

To conclude, in this experiment we have set out to investigate whether we can modify the Turing Test in favour of the computer, by manipulating the computer's responses via the introduction of spelling errors. In some cases our results show that adding spelling errors can have a benefit for the computer, yet it seems chatbots must come a long way still if they are to to give consistent and correct conversational responses, and pass as humans.


\subsection{General Discussion}
In order to zoom out and return to the larger picture, we would like to understand the implications of our findings on the main goal of the current research, namely to evaluate if and how a Turing Test could be used in future Artificial Intelligence research. In order to do this, we will relate the outcome of the experiment to the literature reviewed in the first part of the paper.

We have discussed a number of objections against the test, both practical and theoretical. The theoretical objections can't be refuted by the outcome of the experiment, since those objections are making philosophical rather than empirical claims. Still, we would like to mention them once again in the light of our executed experiment. The first theoretical objection is the Chinese Room argument made by Searle (Bron3), which states that a computer is not capable of real understanding of the symbols it is manipulating.

From the log files of the conversations with the chatbots (see Appendix), we conclude that it is true that the chatbot does not always understand the question which it is given. There were many cases in which the chatbot didn't understand the questions from the interrogator, for instance Example 1.

\begin{figure}[ht]
   \begin{center}
      \fbox{\parbox{\columnwidth}
         {\begin{description}[noitemsep]
               \item [Interrogator] Do you study?
               \item [Rose] I actually just redecorated my study. I had a smart lighting system fitted.
      \end{description}}}
   \end{center}
   \caption{Example 1}
\end{figure}

The answer given by Rose in Example 1 is not the right answer to the question. The question is whether Rose studies, i.e. whether she is a student or whether she is learning a particular subject. The algorithm powering Rose did not catch that, but rather picked up the word \textit{study}, which is a type of room in the house, and constructed an answer related to that. The appropriate question for this answer here would be whether Rose has a study.

But Searle's Chinese Room argument goes further. It says that even when a computer gives a seemingly good reply to the question it still doesn't understand its own reply - in the same way that Sam who is talking Chinese doesn't actually understand Chinese. To make this more concrete, we look at Example 2.

\begin{figure}[ht]
   \begin{center}
      \fbox{\parbox{\columnwidth}
         {\begin{description}[noitemsep]
               \item [Interrogator] What do you think of Trump?
               \item [Mike] He's great. Far less boring than most politicians. And let's face it, government is all about corporate money, so the actual people who take office don't really matter.
      \end{description}}}
   \end{center}
   \caption{Example 2}
\end{figure}

Example 2 shows a correct response from Mike, which could have been the response of a human. Searle's objection is that Mike doesn't really understand what he is saying, since he is only manipulating symbols. This means that even though Mike gives the appropriate response he doesn't know who Trump is and what the government is about, in the same way as that Sam wouldn't know what the Chinese characters stand for.

It seems to be the case that for now Searle is right that computers don't understand the symbols that they are manipulating. There are many examples that the computer gives very strange answers and even if it gives an appropriate answer further questioning often reveals that it actually doesn't understand the words and concepts it is using (like \textit{Trump} or \textit{government}).

This being said, we don't know if Searle will be always right in the future. It is still possible that more elaborated chatbots (or even robots) pass the Turing Test in the future. When that happens, they need to give appropriate answers to the more difficult questions they get and also give appropriate responses to further questions. When that happens, it will be a very strong claim to maintain the idea that the computer don't really understand the symbol it is using.

The second theoretical objection is the simulation objection. This objection states that a simulated \textit{X} is not yet an \textit{X}. However, we have seen that we can distinguish two types of simulation.The first way is when a simulation lacks essential features of whatever is being simulated. The second way is that it is exactly like what is simulated, except that it is hasn't been produced in the natural way. A current discussion point among AI researchers is which of the two ways of simulating is measured by executing the Turing Test.

From the answers from the participants, we can see that some participants thought the computer was the human because it was making spelling errors. However, the chatbot was not actually making those spelling errors himself, but rather was equipped with a program who did that for him. The chatbots itself knows perfectly fine how to write proper english, and would never deliberately make spelling errors when knowing how to write it properly. We could thus argue that, in this case, the Turing Test is measuring the first way of simulating.

The first practical objection is that the Turing Test is too hard for a computer to pass. One of the general reasons for this is that it does not only measure intelligence, but also how human-like the machine is.

A more narrow statement related to Searle's argument is that a computer is not always capable of understanding the human interpretation of the symbols. We will discuss this with some examples as well. However, it might be argued that this human interpretation is still learnable for a computer, but might be harder to learn.

\begin{figure}[ht]
   \begin{center}
      \fbox{\parbox{\columnwidth}
         {\begin{description}[noitemsep]
               \item [Interrogator] What do you like about your town?
               \item [Mike] A town is larger than a village and smaller than a city.
      \end{description}}}
   \end{center}
   \caption{Example 3}
\end{figure}

Humans would expect as an answer to such a question an enumeration of features of the town the person lives in, because the question explicitly states \textit{your town}. The answer from Mike is not entirely incorrect, but would rather be an answer to a question which would use the phrase \textit{living in a town} instead of \textit{your town}.

\begin{figure}[ht]
   \begin{center}
      \fbox{\parbox{\columnwidth}
         {\begin{description}[noitemsep]
               \item [Interrogator] How are you?
               \item [Rose] I'm not good as some, but better than most.
      \end{description}}}
   \end{center}
   \caption{Example 4}
\end{figure}

Although the answer by Rose in Ex. 4 is not really incorrect, it is also not an answer which a human would expect to such a question. An intuitive answer to such a question would be \textit{I'm (fine | ok | not feeling very well)}. However, Rose answers this question from a more broad point of view, rather than basing her answer on her emotional state at the current moment.

We have seen this in Examples 3 and 4, where the answer to the questions were not entirely incorrect, but were not the right answer to the human interpretation of the question.

One of such human features the Turing test does take into account, but has nothing to do with intelligence, is making mistakes. One of such mistakes is discussed in the second alteration, which discusses the introduction of spelling errors in order to make the computer exhibit more human-like behavior. In our experiment, we investigated whether introducing these spelling errors allows a chatbot to perform better in the Turing Test.

We can see from the results that, without spelling errors, 90\% of the participants correctly identified the computer. Therefore, even though Mike and Rose are among the best performing chatbots in the Loebner competition, we are far away of achieving the aimed score of 50\% when using these chatbots in a Turing Test. When using spelling errors, 70\% of the participants correctly identified the computer. The results thus show that when we introduce spelling errors to the chatbot's responses, we get closer to the aimed score of 50\%.

The experiment has shown that the first practical objection is true, i.e. that the Turing Test takes into account human features which have nothing to do with intelligence, which thereby make it more difficult for a computer to pass. The experiment has also shown that we can partly mitigate this objection by implementing the second alteration. Future research should investigate whether using other programs which help the chatbots display more human-like behaviour can help decrease the correct identifications and in doing so assist in mitigating the first theoretical objection.

The second practical objection states that the Turing Test is too narrow for testing intelligence. However, this might be questioned, because the computer must be able to use a variety of tasks including e.g. memory, common knowledge, and personal identity in order to pass the test. We can see evidence of this in the logs of the conversations with the chatbots.

\begin{figure}[ht]
   \begin{center}
      \fbox{\parbox{\columnwidth}
         {\begin{description}[noitemsep]
               \item [Interrogator] How are you?
               \item [Mike] I'm doing fine, thanks. How are you?
               \item [..]
               \item [Interrogator] How are you feeling today?
               \item [Mike] I feel horrible. I think I picked up a bug.
      \end{description}}}
   \end{center}
   \caption{Example 5}
\end{figure}

Example 5 shows how important it is that the chatbot has memory, as otherwise it would give itself away way too easily. Example 6 shows the importance of common knowledge. In the example, the chatbot fortunately has common knowledge, as the chatbot replies with a reasonable answer on Trump. However, if the chatbot did not know who Trump was, it would give its identity away immediately, since, unfortunately, knowing who Trump is nowadays is common knowledge.

\begin{figure}[ht]
   \begin{center}
      \fbox{\parbox{\columnwidth}
         {\begin{description}[noitemsep]
               \item [Interrogator] What do you think of Trump?
               \item [Mike] He's great. Far less boring than most politicians. And let's face it, government is all about corporate money, so the actual people who take office don't really matter.
      \end{description}}}
   \end{center}
   \caption{Example 6}
\end{figure}

The second alteration, which introduces spelling errors, has already been discussed when discussing the first practical objection. The first alteration, which describes the usage of a probability, will now be discussed. In our experiment, the participants were able to identify either one of their conversation partners as the computer, on a probability scale, indicating how certain they were of their choice. The results of the experiment are now more fine-grained, as can be seen in the results section. By using a probability scale, we can now measure the participant's' identification on a ratio scale instead of on a nominal scale.

This discussion of the objections and the alterations shows that by answering the research question with an interdisciplinary approach, we are able to get valuable insights. We have designed and run an experiment so as to validate objections to the Turing Test and to improve upon these objections. Without an empirical component, it would not be possible to give any practical evaluation of the Turing Test. Without a philosophical component, the experiment might not have been related to the current philosophical debate revolving around the Turing Test. By approaching the Turing Test from both a philosophical and an empirical point of view, we are able to provide results upon which a critical discussion can be based.
