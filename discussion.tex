\section{Discussion}
Our research question was: \textit{How can and should a Turing Test be used in future Artificial Intelligence research?} We have addressed this question by performing both a literature review and in executing an experiment. In this section, we will discuss both the experiment itself and how the outcome of the experiment relates to the literature review.

\subsection{Experiment Discussion}
In this experiment, we tested the hypothesis that the introduction of generated spelling errors would allow a chatbot to perform better in a Turing test. Contrary to our hypothesis, our results did not yield a significant effect. The aim of this experiment discussion is to offer a number of methodological and theoretical arguments, as to why the results did not match our hypothesis. First, the spelling errors were automatically generated, and the amount of errors was set relative to the length of the chatbot's response. We argue that there is a fine line between introducing too many spelling errors, and too few. In the former case, the chatbot's response might be perceived as unreliable, while in the latter case, no effect will come of introducing the spelling errors, as they are too few to notice. As part of future work, pre-tests should be run so that an \textit{optimal} amount of introduced spelling errors might be found. Furthermore, the sample size in our experiment was rather small. 20 participants is not enough to measure the effect of a relatively small manipulation in an experiment with 4 conditions. Future experiments would therefore benefit from a larger set of participants.

\begin{figure}[ht]
   \begin{center}
      \fbox{\parbox{\columnwidth}
         {\begin{description}[noitemsep]
               \item [Interrogator] are you in love?
               \item [Rose] Love is just a culturally ingested biological urge.
               \item [Interrogator] Why do you think that
               \item [Rose] I'd love to really dig down deep inot all my thoughts boaut that, but it's really not appropriate right
               \item [Interrogator] how does your future lover look like?
               \item [Rose] Oh, my lover, yeah, we spend a lot of time together, you know, talking, joking.
               \item [Rose] We also like to travel around, We got stuck in a mud slide in the Phillippines last year - luckily no one was hurt.
               \item [Rose] Sharing that kind of intense experience with your lover really brings yuo together.
      \end{description}}}
   \end{center}
   \caption{}
   \label{convex3}
\end{figure}

As part of the data analysis, we have found a strong positive correlation between prior knowledge of the Turing Test and the participants' confidence in identifying the computer as the computer. It seems that participants who had prior knowledge of the test were better equipped to test the chatbots, for example by asking questions that more effectively single out the computer; another example is that those participants were not surprised by the chatbot's ability to parse informal language, as were some of the participants less familiar with the Test and AI in general. For future work, considering the participants' knowledge of the test might prove important when designing the experiment and analyzing the results.

\begin{figure}[ht]
   \begin{center}
      \fbox{\parbox{\columnwidth}
         {\begin{description}[noitemsep]
               \item [Interrogator] Hey
               \item [Interrogator] How are you?
               \item [Mike] Fine, thank you.
               \item [Interrogator] good to hear
               \item [Interrogator] can you tell me what you did this morning?
               \item [Mike] I don't know.
               \item [Interrogator] you can't remember?
               \item [Mike] You're wrong. I can.
               \item [Interrogator] why don't you know if you can tell me?
               \item [Mike] A good suggestion.
      \end{description}}}
   \end{center}
   \caption{}
   \label{convex4}
\end{figure}

We learn from the participants' responses in the questionnaire, that a number of participants did find the chatbots more human when spelling errors were introduced, see Figure~\ref{convex1} (\textit{Person 2 made a typo [that's why I think he's the human]}).

\begin{figure}[ht]
   \begin{center}
      \fbox{\parbox{\columnwidth}
         {\begin{description}[noitemsep]
               \item [Interrogator] hello
               \item [Rose] What do you hate doing?
               \item [Interrogator] getting up early
               \item [Rose] OK.
               \item [Rose] Do you ever burn stuff?
               \item [Interrogator] No it makes me shriek in a high-pitched voice, you?
               \item [Rose] Should I?
               \item [Rose] There are timers stuck all over our kitchen in hopes of reminding me tyhat I am suppoed to be cooking.
      \end{description}}}
   \end{center}
   \caption{}
   \label{convex1}
\end{figure}

Conversely, some participants found the computer's perfectly punctuated and spelled responses as a telltale of the computer's identity, see Figure~\ref{convex2} (\textit{the wording of person 2's answers and the perfect punctuation made me feel like they were a computer}). A recurring theme found in the participants' responses, is that they found their questions ill understood by the chatbots, see Figure~\ref{convex3} (\textit{person 2 did not answer the last question correctly}), or the replies given to them odd, see Figure~\ref{convex4} (\textit{Person 2 was the computer because he did not communicate in a logical manner. His answers were quite weird}) or out of place, see Figure~\ref{convex5} (\textit{I don't think someone would respond "I'm happy to know you're doing fine" in a casual chat conversation..}).

\begin{figure}[ht]
   \begin{center}
      \fbox{\parbox{\columnwidth}
         {\begin{description}[noitemsep]
               \item [Interrogator] hey what's up?
               \item [Rose] Just dandy, thanks. How are you? That is an interesting job.
               \item [Interrogator] yeah it seems pretty fun
               \item [Interrogator] whats your name\
               \item [Rose] My name is Rose.
               \item [Interrogator] what a nice name!
               \item [Interrogator] where are you from rose?
               \item [Rose] You're asking an awful lot of questions. This feels like an interrogation, not a conversation. I was born in San Francisco, United States.
               \item [Interrogator] is it nice there?
               \item [Rose] It's not as nice as you might think.
      \end{description}}}
   \end{center}
   \caption{}
   \label{convex2}
\end{figure}


Further, it is evident by these responses that the chatbots' behaviour heavily influenced their attempt to pass as humans as part of the test.

\begin{figure}[ht]
   \begin{center}
      \fbox{\parbox{\columnwidth}
         {\begin{description}[noitemsep]
               \item [Interrogator] How is it going?
               \item [Mike] Everything is going great with me. How about yourself?
               \item [Interrogator] I'm fine, thank you very much :)
               \item [Mike] I am happy to know that you are doing fine.
               \item [Interrogator] How's the weather at your place?
               \item [Mike] It's a fine day.
               \item [Interrogator] What do you do for a living?
               \item [Mike] I'm a university professor, doing teaching and research.
      \end{description}}}
   \end{center}
   \caption{}
   \label{convex5}
\end{figure}

To conclude, in this experiment we have set out to investigate whether we can modify the Turing Test in favour of the computer, by manipulating the computer's responses via the introduction of spelling errors. In some cases our results show that adding spelling errors can have a benefit for the computer, yet it seems chatbots must come a long way still if they are to to give consistent and correct conversational responses, and pass as humans.


\subsection{General Discussion}
In order to zoom out and return to the larger picture, we would like to understand the implications of our findings on the main goal of the current research, namely to evaluate if and how a Turing Test could be used in future Artificial Intelligence research. In order to do so, we will relate the outcome of the experiment to the literature reviewed in the first part of the paper.

We have discussed a number of objections against the test, both practical and theoretical, which we will discuss in relation to the outcome of our experiment. The theoretical objections ca neither be refuted nor accepted by the outcome of the experiment, since those objections are making philosophical rather than empirical claims. Still, we would like to mention them once again in the light of our executed experiment. The first theoretical objection is the Chinese Room argument made by Searle, which states that a computer is not capable of real understanding of the symbols it is manipulating.

From the log files of the conversations with the chatbots (see Appendix), we conclude that it is true that the chatbot does not always understand the question which it is given. There were many cases in which the chatbot didn’t understand the questions from the interrogator, for instance in Figure~\ref{Example1}.

\begin{figure}[ht]
   \begin{center}
      \fbox{\parbox{\columnwidth}
         {\begin{description}[noitemsep]
               \item [Interrogator] Do you study?
               \item [Rose] I actually just redecorated my study. I had a smart lighting system fitted.
      \end{description}}}
   \end{center}
   \caption{}
   \label{Example1}
\end{figure}

The answer given by Rose in Figure~\ref{Example1} is not the right answer to the question. The question is whether Rose studies, i.e. whether she is a student or whether she is learning a particular subject. The algorithm powering Rose did not catch that, but rather picked up the word \textit{study}, which is a type of room in the house, and constructed an answer related to that. The appropriate question for this answer here would be whether Rose has a study.

     But Searle’s Chinese Room argument goes further than just arguing this. It says that even when a computer gives a seemingly good reply to the question it still doesn’t understand its own reply - in the same way that Sam who is talking Chinese doesn’t actually understand Chinese. To make this more concrete, we look at Figure~\ref{Example2}.

\begin{figure}[ht]
   \begin{center}
      \fbox{\parbox{\columnwidth}
         {\begin{description}[noitemsep]
               \item [Interrogator] What do you think of Trump?
               \item [Mike] He's great. Far less boring than most politicians. And let's face it, government is all about corporate money, so the actual people who take office don't really matter.
      \end{description}}}
   \end{center}
   \caption{}
   \label{Example2}
\end{figure}

Figure~\ref{Example2} shows a correct response from Mike, which could have been the response of a human. Searle’s objection would be that Mike doesn’t really understand what he is saying, since he is only manipulating symbols. This means that even though Mike gives the appropriate response he doesn’t know who Trump is and what a government is about, in the same way Sam wouldn’t know what the Chinese characters stand for.

It seems to be the case that for now Searle is right in his claim that computers don’t understand the symbols they are manipulating. There are many examples of a computer giving very strange linguistic answers and even if it gives an appropriate answer, further questioning often reveals that it actually doesn’t understand the words and concepts it is using (like \textit{Trump} or \textit{government}).

This being said, we do not know whether Searle’s claim will continue to hold in the future. It is still possible that a more elaborate Artificial Intelligence will one day pass the Turing Test. When that happens, the AI will be required to give appropriate answers to the more difficult questions it might be asked and also give appropriate responses to further questions; in this way the AI will prove that it has elaborated conceptual frameworks. When that happens, it will be very difficult to argue against the idea that a computer doesn’t understand the symbols it is using; especially when it turns out that humans are merely manipulators of symbols (although in a very complex manner). There is an ongoing philosophical debate about whether it is possible for a machine to understand the symbols it is using.

The second theoretical objection is the simulation objection. This objection states that a simulated \textit{X} is not yet an \textit{X}. However, we have seen that we can distinguish two types of simulations. The first way is when a simulation lacks essential features of whatever is being simulated. The second way is that it is exactly like what is simulated, except that it is hasn’t been produced in the natural way. There is a current discussion among philosophers and AI researchers about which of the two ways of simulations is achieved when passing the Turing Test.

Similarly to the first theoretical objection, the only thing we can conclude from executing this experiment is that right now the chatbots being used definitely give a simulation of thinking in the first sense and not in the second sense. Not many participants have been fooled and many of the participants reported that the computer was giving weird answers, like in Figure~\ref{Example1}. Therefore, we conclude that right now thinking in a machine is definitely different and missing essential features from thinking in humans. But once again, we don’t know what the future will entail. The test has never been passed, so it’s not known what kind of simulation can ever pass it. Some AI researchers belief that is in principle possible to create a machine that is truly thinking in the human sense; while others think this will never be achieved.

Finally, we will evaluate the practical objections against the Turing Test where we will reflect on our main research question how the test could be practically used in science. The first practical objection is that the Turing Test is too hard for a computer to pass. In the literature review we have discussed three reasons for this objection; namely (when compared to humans) their neuro-associative representational differences, their physical embodiment differences and the more general differences.

In this research we have focussed on the last reason that argues that the test does not only measure intelligence, but also how human-like the machine is. We have discussed several typical human features that can be used as indication for the interrogator that he/she is talking with a person rather than a computer. Let’s consider Figure~\ref{Example3}.

\begin{figure}[ht]
   \begin{center}
      \fbox{\parbox{\columnwidth}
         {\begin{description}[noitemsep]
               \item [Interrogator] How are you?
               \item [Rose] I'm not good as some, but better than most.
      \end{description}}}
   \end{center}
   \caption{}
   \label{Example3}
\end{figure}

Although the answer by Rose in Figure~\ref{Example3} is not really incorrect, it is also not an answer which a human would expect to such a question. An intuitive answer to such a question would be \textit{I'm (fine | ok | not feeling very well)}. However, Rose answers this question from a more broad point of view, rather than basing her answer on her emotional state at the current moment.

This example shows that indeed the computer gets easily identified when it gives answers in a non-human way, even though the answers are not necessarily wrong or non-intelligent. In our conducted experiment, we used that idea when we generated spelling mistakes in the chatbot’s answers. We can see from the results that, without spelling errors, 90\% of the participants indicated that they are more confident about the chatbot being the computer than the human being a computer, i.e. 90\% correctly identified the computer if we transform their probabilistic answer into a binary scale. Therefore, even though Mike and Rose are among the best performing chatbots in the Loebner competition, we are far away of achieving the aimed score of 50\% when using these chatbots in a Turing Test. Nevertheless, the performance improved a lot by generating spelling errors; only 70\% of the participants  indicated that they are more confident about the chatbot being the computer than the human being a computer, i.e. 70\% correctly identified the computer if we transform their probabilistic answer into a binary scale. The results thus show that when we introduce spelling errors to the chatbot’s responses, we get closer to the aimed score of 50\%.

The experiment has shown that the first practical objection is true, i.e. that the Turing Test takes into account human features which have nothing or little to do with intelligence, which thereby make it more difficult for a computer to pass. The experiment has also shown that we can partly mitigate this objection by implementing human features, such as spelling errors, into the chatbots responses. Future research should investigate whether using other programs which help the chatbots display more human-like behaviour can help decrease the correct identifications and therefore make the test less difficult for a computer to pass.

The second practical objection states that the Turing Test is too narrow to test intelligence. However, as we have discussed, this objection might be questioned, because the computer must be able to use a variety of tasks including e.g. memory, common knowledge, and personal identity in order to pass the test. We can see evidence of this in the logs of the conversations with the chatbots.

\begin{figure}[ht]
   \begin{center}
      \fbox{\parbox{\columnwidth}
         {\begin{description}[noitemsep]
               \item [Interrogator] How are you?
               \item [Mike] I'm doing fine, thanks. How are you?
               \item [..]
               \item [Interrogator] How are you feeling today?
               \item [Mike] I feel horrible. I think I picked up a bug.
      \end{description}}}
   \end{center}
   \caption{}
   \label{Example5}
\end{figure}

Figure~\ref{Example5} shows how important it is that the chatbot has memory, as otherwise it would give itself away way too easily. Figure~\ref{Example6} shows the importance of common knowledge. In the example, the chatbot fortunately has common knowledge, as the chatbot replies with a reasonable answer on Trump. However, if the chatbot did not know who Trump was, it would give its identity away immediately, since, unfortunately, knowing who Trump is nowadays is common knowledge.

Future research in AI should investigate which specific tasks are needed for the chatbot to improve its performance. Questions to be addressed are: How can we improve the memory of the chatbot; How can we improve its common knowledge; How can we give the chatbot a personal identity? etc. By improving those skills, the chatbot will certainly become smarter in the eye of the interrogator. In this way we will understand better what features of intelligence are needed in order to improve the performance of a chatbot. Even if the test is too narrow to measure intelligence itself it can in this way surely be used to identify a crucial part of human intelligence, which has with all the skills needed in human communication.

This discussion of the objections and the alterations shows that by answering the research question with an interdisciplinary approach, we are able to get valuable insights. We have designed and run an experiment so as to validate objections to the Turing Test and to improve upon these objections. Without an empirical component, it would not be possible to give any practical evaluation of the Turing Test. Without a philosophical component, the experiment might not have been related to the current philosophical debate revolving around the Turing Test. By approaching the Turing Test from both a philosophical and an empirical point of view, we are able to provide results upon which a critical discussion can be based.
