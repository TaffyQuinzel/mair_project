\section{From Literature To Experiment}
To evaluate the validity of the Turing Test even more, we now turn to our own conducted experiment. As we have seen, there are ways to alter the original Turing Test so that the computer gets the chance to achieve a better performance. One of the ways we have discussed is to make the computer more human-like, even though it would not necessarily make the computer more intelligent. In our experiment we will focus on this suggestion. We will investigate this suggestion by generating spelling errors into the computer’s answers. Spelling errors are typically seen as human mistakes, while computers are thought to be excellent spellers. By adding spelling errors, we excluding the possibility that the chatbot’s identity will be easily identified by its excellent spelling - and in this way give a chatbot a fair chance in participating in a Turing Test. Moreover, in line with the first alteration that we have discussed, we will also measure the participant’s belief about which of the two conversation partners is a computer on a probability scale. We measure the outcome on a ratio scale instead of on a simple nominal scale.

When designing or executing a Turing Test, the output criterion~\cite{copeland2015artificial} is important to take into account. The output criterion states that the interrogator should talk to the human and the machine simultaneously, in order to be able to compare the conversational output of them both when assessing which is the human and which is the machine. Therefore, we make sure this criterion is satisfied in our execution of the test.

